# Service Control Policy (SCP) Management Pipeline

This repository provides an IaC management solution for Service Control Policies (SCPs). The repository can be used to quickly migrate from manually-managed SCPs. **This solution DOES NOT include/import the SCPs created by AWS Control Tower as Control tower Guardrails**.

This particular model is forked from `https://github.com/aws-samples/scp-management-reference-architecture`. This model uses a more visual structure for SCP management and uses Python to dynamically create Terraform resource statements, rather than having maintainers update the Terraform code directly.

# Code Walk-through

## Repository Structure

```sh
.
├── bootstrap # <-- Terraform module for creating Terraform backend configuration (state bucket, lock table). Does not need to be run if you have already bootstrapped the environment.
├── find_blocking_scp # <-- a folder with a script that helps troubleshoot which SCP is blocking an action.
├── scp_module               # <-- Terraform module for creating SCPs and attaching them to targets
├── service_control_policies # <-- a directory with sub-directories specific to the OUs to which SCPs are directly attached
├    ├─── ROOT # <-- Directory containing the full OU structure
├    └─── SHARED # <-- Folder containing SCP JSON files that are intended to be attached to multiple targets
├── backend.tf # <-- Contains information about where TF should store its backend configuration (state bucket, lock table). Needs to be updated prior to first pipeline run.
├── (Python Scripts) # <-- See Scripts section
├── (scp_define_attach_auto.tf)     # <-- [NOT PRESENT IN CODE, WILL BE AUTO-GENERATED IN PIPELINE] main terraform file that is dynamically generated by the resolve_scp_data.py script.
├── README.md                # <-- This file
└── scp-pipeline.yaml # <-- CloudFormation template used to create the SCP CodePipeline - can be removed if you have an existing pipeline solution
```

### Scripts

This solution uses Python to generate Terraform content.

1. **_`generate_scp_ou_structure_and_imports.py`_** - This script is used to set up the initial OU structure and Terraform import files. It helps accelerate the use of this solution. It should be run manually when first implementing the solution, as well as any time there are OU structure changes (eg. new accounts added, OU renames, OU restructures).
2. **_`resolve_scp_data.py`_** - This script runs during the pipeline execution. It should not be manually run. This script will dynamically generate Terraform code based on the contents of the `service_control_policies` OU structure folder.

## SCP File Naming Convention

> NOTE:
>
> 1. All SCP files created in this repository are JSON files (`.json` extension) -- this is to match the output that will be visible in the AWS console.
> 2. [Future] If you need to use Terraform template files instead, use the extension `.json.tpl`. Terraform template files allow for dynamic substitution of string data (eg. if I need to evaluate an account ID from a variable at Terraform Plan time). The `resolve_scp_data.py` script currently does not support template files, but may support them in a future version.

# Steps to manage SCPs

## Managing this repository

Before making changes, validate that the SCP statement(s) you are adding are not already implemented in another policy.

## Adding new SCPs

1. To add a new SCP, you will need to create a JSON file in the appropriate location, with a `policy` key containing the policy and a `description` key containing a description of the policy. It is generally advisable to include some explanation of what statements do or what change tickets they are associated with.

2. If the SCP is used in just one OU or account, create the JSON file directly in the folder for that OU/account. If the SCP is used in multiple locations, create it in the `service_control_policies/SHARED` folder and create placeholder files (eg. `security_baseline.placeholder.shared`) in each location where it will be attached (short justification: placeholders help avoid exceeding the SCP-OU attachment limit of 5).

## Modifying existing SCPs

1. Modify the JSON file. You can add, remove, or edit statements and the pipeline will update them in place.
2. If you are applying a single-use SCP to another OU/Account, you will need to move the policy to the `SHARED` directory and make references to it using a `.shared` file.

## Removing/Detaching SCPs

1. To detach an SCP, simply delete the JSON file.
2. If all attachments of an SCP are removed, the SCP itself will also be removed. SCPs can be stored in the code repository `SHARED` folder but will not be created without an attachment.

## Handling OU restructures

1. Re-run the `generate_scp_ou_structure_and_import.py` script and modify as necessary to align SCPs to the new structure.

# Initial Implementation Steps

1. Determine if you want to run this in the management or Organizations delegated admin account. Generally, you should use a delegated administrator when possible. This will be called the "SCP Account" in later steps
   1. If using a delegated administrator account, you will need to make sure that the Organizations service gives appropriate permissions to the account. This is an example policy to provide SCP management permissions in the Organizations->Settings menu (replace XXXX with your delegated administrator account ID):
   ```json
   {
     "Version": "2012-10-17",
     "Statement": [
       {
         "Sid": "ViewAWSOrganizationsResources",
         "Effect": "Allow",
         "Principal": {
           "AWS": "arn:aws:iam::XXXXXXXXXXXX:root"
         },
         "Action": [
           "organizations:DescribeOrganization",
           "organizations:DescribeOrganizationalUnit",
           "organizations:DescribeAccount",
           "organizations:DescribePolicy",
           "organizations:DescribeEffectivePolicy",
           "organizations:ListRoots",
           "organizations:ListOrganizationalUnitsForParent",
           "organizations:ListParents",
           "organizations:ListChildren",
           "organizations:ListAccounts",
           "organizations:ListAccountsForParent",
           "organizations:ListPolicies",
           "organizations:ListPoliciesForTarget",
           "organizations:ListTargetsForPolicy",
           "organizations:ListTagsForResource"
         ],
         "Resource": "*"
       },
       {
         "Sid": "DelegatingAllActionsForServiceControlPolicies",
         "Effect": "Allow",
         "Principal": {
           "AWS": "arn:aws:iam::XXXXXXXXXXXX:root"
         },
         "Action": [
           "organizations:CreatePolicy",
           "organizations:UpdatePolicy",
           "organizations:DeletePolicy",
           "organizations:AttachPolicy",
           "organizations:DetachPolicy",
           "organizations:EnablePolicyType",
           "organizations:DisablePolicyType",
           "organizations:TagResource",
           "organizations:UntagResource"
         ],
         "Resource": [
           "arn:aws:organizations::*:policy/*/service_control_policy/*",
           "arn:aws:organizations::*:account/*/*",
           "arn:aws:organizations::*:ou/*/*",
           "arn:aws:organizations::*:root/*/*"
         ]
       }
     ]
   }
   ```
   For more details, see https://medium.com/cloud-security/delegating-scp-management-to-governance-team-via-aws-organizations-53334a31b71c
2. Using CLI credentials from the SCP account, run the script `generate_scp_ou_structure_and_import.py`
   1. This script will generate the following content:
      1. A `service_control_policies` folder that contains a folder structure mirroring the OU structure of the Organization. Within this folder, there are two subfolders: `ROOT` and `SHARED`. Within these folders are JSON files with Service Control Policies.
      2. Two import manifest (`.tf`) files, one for SCP imports and one for SCP attachment imports.
3. Configure a pipeline to run this solution
   1. [Option 1] If you need to deploy a pipeline specifically for this solution and don't have a standard pipeline solution for your organization: Deploy the CloudFormation template `scp-pipeline.yaml` to create a pipeline that uses Terraform to manage SCPs.
      1. You may need to update the template's KMS policy to explicitly give the role creating the CloudFormation Stack admin access to the key.
      2. If you want to use a different source, such as BitBucket or GitHub, you will need to modify the CFT to use a CodeStar connection resource.
      3. Deploying the CFT requires at least the following permissions (may require changes if you use non-default arguments):
      ```json
      {
        "Version": "2012-10-17",
        "Statement": [
          {
            "Sid": "CreateKmsKeys",
            "Effect": "Allow",
            "Action": ["kms:CreateKey", "kms:ListAliases"],
            "Resource": "*"
          },
          {
            "Effect": "Allow",
            "Action": ["kms:CreateAlias", "kms:DeleteAlias"],
            "Resource": ["arn:aws:kms:*:*:alias/scp-*", "arn:aws:kms:*:*:key/*"]
          },
          {
            "Effect": "Allow",
            "Action": [
              "codestar-connections:CreateConnection",
              "codestar-connections:DeleteConnection"
            ],
            "Resource": "arn:aws:codestar-connections:*:*:connection/*"
          },
          {
            "Effect": "Allow",
            "Action": [
              "codepipeline:CreatePipeline",
              "codepipeline:DeletePipeline"
            ],
            "Resource": [
              "arn:aws:codepipeline:*:*:scp-pipeline",
              "arn:aws:codepipeline:*:*:scp-pipeline/*"
            ]
          },
          {
            "Effect": "Allow",
            "Action": ["codebuild:CreateProject", "codebuild:DeleteProject"],
            "Resource": "arn:aws:codebuild:*:*:project/scp-*"
          },
          {
            "Effect": "Allow",
            "Action": [
              "iam:CreateRole",
              "iam:UpdateRole",
              "iam:DeleteRole",
              "iam:AttachRolePolicy",
              "iam:DetachRolePolicy",
              "iam:GetRolePolicy",
              "iam:PutRolePolicy"
            ],
            "Resource": "arn:aws:iam::*:role/scp-*"
          }
        ]
      }
      ```
   2. [Option 2] If you DO have a standard pipeline solution, you will need to ensure that you run the script `resolve_scp_data.py` before any `terraform plan/apply` actions. This ensures that the JSON data and folder structure gets translated into attached SCPs.
      1. The minimum permissions for a bring-your-own pipeline are:
      ```json
      {
        "Version": "2012-10-17",
        "Statement": [
          {
            "Sid": "orgReadPermissions",
            "Effect": "Allow",
            "Action": [
              "organizations:DescribeOrganization",
              "organizations:DescribeOrganizationalUnit",
              "organizations:DescribeAccount",
              "organizations:DescribePolicy",
              "organizations:DescribeEffectivePolicy",
              "organizations:ListRoots",
              "organizations:ListOrganizationalUnitsForParent",
              "organizations:ListParents",
              "organizations:ListChildren",
              "organizations:ListAccounts",
              "organizations:ListAccountsForParent",
              "organizations:ListPolicies",
              "organizations:ListPoliciesForTarget",
              "organizations:ListTargetsForPolicy",
              "organizations:ListTagsForResource"
            ],
            "Resource": "*"
          },
          {
            "Sid": "orgWritePermissions",
            "Effect": "Allow",
            "Action": [
              "organizations:CreatePolicy",
              "organizations:UpdatePolicy",
              "organizations:DeletePolicy",
              "organizations:AttachPolicy",
              "organizations:DetachPolicy",
              "organizations:EnablePolicyType",
              "organizations:DisablePolicyType"
            ],
            "Resource": [
              "arn:aws:organizations::*:policy/*/service_control_policy/*",
              "arn:aws:organizations::*:account/*/*",
              "arn:aws:organizations::*:ou/*/*",
              "arn:aws:organizations::*:root/*/*"
            ]
          }
        ]
      }
4. [Optional] If you do not already have a Terraform state bucket and Terraform DynamoDB lock table for backending Terraform, use the `tf_state_bucket_bootstrap` folder to deploy those resources. You will need permissions to create an S3 bucket and a DynamoDB table for this.
5. Once you have a TF state bucket, ensure that the `backend.tf` file points to your Terraform state bucket.
6. Update your repository to include the initial files (including those generated by `generate_scp_ou_structure_and_import.py`) using `git`. Make sure you include the `service_control_policies` directory, `scp_module` directory, import manifests, `backend.tf` manifest, and `resolve_scp_data.py` file at a minimum.
7. Follow along with the pipeline execution, fix any errors, and manually approve the changes if the `terraform plan` aligns with your desired state of the environment.
8.  After the first successful `apply` of the pipeline, you may remove the `import` files or rename them (eg. to `.tf.bak`) for historical reference.
9.  Make changes to SCPs using this solution and don't make any more changes using the console!
   1. For inspiration on SCPs to apply to your environment, check out: https://aws-samples.github.io/aws-iam-permissions-guardrails/guardrails/scp-guardrails.html
   2. If using a delegated administrator, you can also use an SCP to deny non-pipeline roles from modifying SCPs. Note that this restriction will not affect the management account.
10. [Optional] If you deployed the `scp-pipeline.yaml`, you may update the pipeline to remove the manual approval step if/when you are comfortable with removing it. If you do this, you can mitigate risk by creating a [pipeline for each feature branch](https://aws.amazon.com/blogs/devops/multi-branch-codepipeline-strategy-with-event-driven-architecture/) that runs just the SCP resolve Python script and Terraform plan. This will tell you what changes will occur so that you can determine whether to merge them into the `main` branch.
